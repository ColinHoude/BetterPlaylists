{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9838164,"sourceType":"datasetVersion","datasetId":6035063},{"sourceId":9838538,"sourceType":"datasetVersion","datasetId":6035309}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom transformers import ASTFeatureExtractor, ASTModel\nimport torchaudio\nimport os\n\n# -------------------------------------------------------------------------- #\n# ------------------------------ Model Loading ----------------------------- #\n# -------------------------------------------------------------------------- #\n\n# Load the feature extractor and model\nfeature_extractor = ASTFeatureExtractor.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\nmodel = ASTModel.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n\n\ndef load_audio(file_path):\n    waveform, sample_rate = torchaudio.load(file_path)\n    return waveform, sample_rate\n\n\ndef get_embeddings(file_path):\n    # Load audio\n    waveform, sample_rate = load_audio(file_path)\n    \n    # Resample to 16 kHz if not already\n    if sample_rate != 16000:\n        waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(waveform)\n    \n    # Preprocess to match model input\n    inputs = feature_extractor(waveform.squeeze().numpy(), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n    \n    # Move model and inputs to GPU if available\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    inputs = {key: value.to(device) for key, value in inputs.items()}\n    \n    # Get embeddings\n    with torch.no_grad():\n        outputs = model(**inputs)\n        embeddings = outputs.last_hidden_state.mean(dim=1).squeeze()  # Averaging across time dimension\n        \n    return embeddings.cpu()  # Move embeddings back to CPU if needed\n\n\nfolder_path = \"/kaggle/input/testingembeddings/TestingEmbedding\"\n\n# Loop through the audio files and extract embeddings\nembeddings_dict = {}\nfor file_name in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, file_name)\n    if file_path.endswith('.mp3') or file_path.endswith('.wav'):  # Check audio format\n        embeddings = get_embeddings(file_path)\n        embeddings_dict[file_name] = embeddings\n\n\nprint(embeddings_dict)","metadata":{"execution":{"iopub.status.busy":"2024-11-08T00:56:26.386387Z","iopub.execute_input":"2024-11-08T00:56:26.387306Z","iopub.status.idle":"2024-11-08T00:56:58.204481Z","shell.execute_reply.started":"2024-11-08T00:56:26.387252Z","shell.execute_reply":"2024-11-08T00:56:58.203466Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/297 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cf4ff5b180d475591fcc61a3c3db85b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74b053b6991141deb9f7197400bd4427"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97b999b0acbc4a1081d1cb20f1ee5492"}},"metadata":{}},{"name":"stdout","text":"{'House (3).mp3': tensor([[ 0.7692, -0.7077,  0.5604,  ..., -0.2571, -0.4534,  0.5017],\n        [ 0.7342, -0.6028,  0.6345,  ..., -0.2790, -0.4677,  0.6714]]), 'DeepHouse1 (1).mp3': tensor([[ 0.9456,  0.3324,  0.1903,  ..., -0.2377, -0.9160,  1.0630],\n        [ 0.9730,  0.2654,  0.2404,  ..., -0.3548, -0.9983,  0.9740]]), 'House (1).mp3': tensor([[ 1.5623, -0.1902, -0.3728,  ..., -0.4106, -0.1305,  0.9027],\n        [ 1.4673, -0.1913, -0.1964,  ..., -0.5215, -0.1411,  0.8944]]), 'AfroHouse1.mp3': tensor([[ 0.6066, -0.2492, -0.4886,  ..., -0.5231, -0.0573,  0.4633],\n        [ 0.5589, -0.3098, -0.3416,  ..., -0.5105, -0.1697,  0.6014]]), 'AfroHouse2.mp3': tensor([[ 0.9118, -0.0214,  0.4618,  ..., -0.9112, -0.6064,  0.6468],\n        [ 0.9019, -0.0609,  0.3931,  ..., -0.9399, -0.6030,  0.6325]]), 'House (2).mp3': tensor([[ 0.7855, -0.2513,  0.4842,  ..., -0.5188, -0.4897,  1.0515],\n        [ 0.7929, -0.2414,  0.4718,  ..., -0.4597, -0.5172,  0.9927]]), 'MelodicHouse (3).mp3': tensor([[ 0.9267, -0.3868,  0.6492,  ..., -0.9390, -0.0382,  0.8170],\n        [ 0.8940, -0.2743,  0.6016,  ..., -0.8947,  0.0338,  0.9520]]), 'DeepHouse1 (3).mp3': tensor([[ 0.7217, -0.0841,  0.6400,  ..., -0.1819, -0.6607,  1.0654],\n        [ 0.7654, -0.2227,  0.8353,  ..., -0.2846, -0.7059,  1.0324]]), 'DeepHouse1 (2).mp3': tensor([[ 0.7871,  0.2734,  0.0838,  ..., -0.4976, -0.4025,  0.8486],\n        [ 0.8370,  0.0432,  0.3677,  ..., -0.5046, -0.2822,  0.6760]]), 'MelodicHouse (2).mp3': tensor([[ 0.5025,  0.1559,  0.1240,  ..., -0.6317, -0.2955,  1.1975],\n        [ 0.5379,  0.0756,  0.1616,  ..., -0.6221, -0.3212,  1.2496]]), 'AfroHouse3.mp3': tensor([[ 0.8936, -0.0132,  0.4426,  ..., -0.9193, -0.5504,  0.6308],\n        [ 0.8957, -0.0582,  0.3847,  ..., -0.9452, -0.5997,  0.5909]]), 'MelodicHouse (1).mp3': tensor([[ 0.7527, -0.2467,  0.5064,  ..., -0.4523, -0.7064,  0.6033],\n        [ 0.7921, -0.3626,  0.5043,  ..., -0.4621, -0.6040,  0.5759]])}\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\ndef find_most_similar_audio(embeddings_dict):\n    # Convert dictionary of embeddings to lists for easy indexing\n    file_names = list(embeddings_dict.keys())\n    \n    # Average embeddings along the first dimension to get shape [768] for each file\n    embeddings = [embedding.mean(dim=0) for embedding in embeddings_dict.values()]\n    \n    # Store most similar files and similarity scores\n    most_similar_files = {}\n    \n    # Loop through each file to find the most similar one\n    for i in range(len(embeddings)):\n        current_embedding = embeddings[i]  # Select the current embedding\n        max_similarity = -1  # Initialize to minimum similarity\n        most_similar_file = None\n        \n        # Compute similarity with all other files\n        for j in range(len(embeddings)):\n            if i != j:  # Skip comparison with itself\n                # Compute cosine similarity for embeddings with shape [768]\n                similarity = torch.cosine_similarity(current_embedding, embeddings[j], dim=0).item()\n                \n                # Update most similar file if similarity is higher\n                if similarity > max_similarity:\n                    max_similarity = similarity\n                    most_similar_file = file_names[j]\n        \n        # Store result for the current file\n        most_similar_files[file_names[i]] = (most_similar_file, max_similarity)\n    \n    # Display results\n    for file, (similar_file, similarity) in most_similar_files.items():\n        print(f\"The most similar file to '{file}' is '{similar_file}' with a similarity score of {similarity:.4f}\")\n\n# Usage\nfind_most_similar_audio(embeddings_dict)","metadata":{"execution":{"iopub.status.busy":"2024-11-08T01:13:56.018860Z","iopub.execute_input":"2024-11-08T01:13:56.020054Z","iopub.status.idle":"2024-11-08T01:13:56.037907Z","shell.execute_reply.started":"2024-11-08T01:13:56.019988Z","shell.execute_reply":"2024-11-08T01:13:56.036992Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"The most similar file to 'House (3).mp3' is 'MelodicHouse (1).mp3' with a similarity score of 0.9357\nThe most similar file to 'DeepHouse1 (1).mp3' is 'House (2).mp3' with a similarity score of 0.9402\nThe most similar file to 'House (1).mp3' is 'AfroHouse1.mp3' with a similarity score of 0.8859\nThe most similar file to 'AfroHouse1.mp3' is 'MelodicHouse (2).mp3' with a similarity score of 0.8984\nThe most similar file to 'AfroHouse2.mp3' is 'AfroHouse3.mp3' with a similarity score of 0.9989\nThe most similar file to 'House (2).mp3' is 'DeepHouse1 (3).mp3' with a similarity score of 0.9541\nThe most similar file to 'MelodicHouse (3).mp3' is 'AfroHouse3.mp3' with a similarity score of 0.9187\nThe most similar file to 'DeepHouse1 (3).mp3' is 'House (2).mp3' with a similarity score of 0.9541\nThe most similar file to 'DeepHouse1 (2).mp3' is 'AfroHouse1.mp3' with a similarity score of 0.8624\nThe most similar file to 'MelodicHouse (2).mp3' is 'AfroHouse1.mp3' with a similarity score of 0.8984\nThe most similar file to 'AfroHouse3.mp3' is 'AfroHouse2.mp3' with a similarity score of 0.9989\nThe most similar file to 'MelodicHouse (1).mp3' is 'House (2).mp3' with a similarity score of 0.9385\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-11-07T23:56:01.919777Z","iopub.execute_input":"2024-11-07T23:56:01.920535Z","iopub.status.idle":"2024-11-07T23:56:01.932861Z","shell.execute_reply.started":"2024-11-07T23:56:01.920492Z","shell.execute_reply":"2024-11-07T23:56:01.931975Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"{'House (3).mp3': tensor([[ 0.7692, -0.7077,  0.5604,  ..., -0.2571, -0.4534,  0.5017],\n        [ 0.7342, -0.6028,  0.6345,  ..., -0.2790, -0.4677,  0.6714]]), 'DeepHouse1 (1).mp3': tensor([[ 0.9456,  0.3324,  0.1903,  ..., -0.2377, -0.9160,  1.0630],\n        [ 0.9730,  0.2654,  0.2404,  ..., -0.3548, -0.9983,  0.9740]]), 'House (1).mp3': tensor([[ 1.5623, -0.1902, -0.3728,  ..., -0.4106, -0.1305,  0.9027],\n        [ 1.4673, -0.1913, -0.1964,  ..., -0.5215, -0.1411,  0.8944]]), 'AfroHouse1.mp3': tensor([[ 0.6066, -0.2492, -0.4886,  ..., -0.5231, -0.0573,  0.4633],\n        [ 0.5589, -0.3098, -0.3416,  ..., -0.5105, -0.1697,  0.6014]]), 'AfroHouse2.mp3': tensor([[ 0.9118, -0.0214,  0.4618,  ..., -0.9112, -0.6064,  0.6468],\n        [ 0.9019, -0.0609,  0.3931,  ..., -0.9399, -0.6030,  0.6325]]), 'House (2).mp3': tensor([[ 0.7855, -0.2513,  0.4842,  ..., -0.5188, -0.4897,  1.0515],\n        [ 0.7929, -0.2414,  0.4718,  ..., -0.4597, -0.5172,  0.9927]]), 'MelodicHouse (3).mp3': tensor([[ 0.9267, -0.3868,  0.6492,  ..., -0.9390, -0.0382,  0.8170],\n        [ 0.8940, -0.2743,  0.6016,  ..., -0.8947,  0.0338,  0.9520]]), 'DeepHouse1 (3).mp3': tensor([[ 0.7217, -0.0841,  0.6400,  ..., -0.1819, -0.6607,  1.0654],\n        [ 0.7654, -0.2227,  0.8353,  ..., -0.2846, -0.7059,  1.0324]]), 'DeepHouse1 (2).mp3': tensor([[ 0.7871,  0.2734,  0.0838,  ..., -0.4976, -0.4025,  0.8486],\n        [ 0.8370,  0.0432,  0.3677,  ..., -0.5046, -0.2822,  0.6760]]), 'MelodicHouse (2).mp3': tensor([[ 0.5025,  0.1559,  0.1240,  ..., -0.6317, -0.2955,  1.1975],\n        [ 0.5379,  0.0756,  0.1616,  ..., -0.6221, -0.3212,  1.2496]]), 'AfroHouse3.mp3': tensor([[ 0.8936, -0.0132,  0.4426,  ..., -0.9193, -0.5504,  0.6308],\n        [ 0.8957, -0.0582,  0.3847,  ..., -0.9452, -0.5997,  0.5909]]), 'MelodicHouse (1).mp3': tensor([[ 0.7527, -0.2467,  0.5064,  ..., -0.4523, -0.7064,  0.6033],\n        [ 0.7921, -0.3626,  0.5043,  ..., -0.4621, -0.6040,  0.5759]])}\n","output_type":"stream"}]}]}